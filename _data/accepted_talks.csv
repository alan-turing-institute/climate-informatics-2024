Name,Date,order,TalkTitle,Abstract,
Tamsin Edwards,22 April 2024,1,How uncertain is future sea level rise?,Keynote,
Isabelle Tingzon,22 April 2024,2,Mapping Housing Stock Characteristics from Drone Images for Climate Resilience in the CaribbeanAnnotations via NIDM-Terms Fosters Improved Search of OpenNeuro Datasets,"Comprehensive housing stock information is crucial for informing the development of climate-resilience strategies aiming to reduce the adverse impacts of extreme climate hazards in high-risk regions like the Caribbean. In this study, we propose an end-to-end workflow for rapidly generating critical baseline exposure data using very high-resolution drone imagery and deep learning techniques. Specifically, our work leverages the Segment Anything Model (SAM) and convolutional neural networks (CNN) to automate the generation of building footprints and roof classification maps. We evaluate the cross-country generalizability of the CNN models to determine how well models trained in one geographical context can be adapted to another. Finally, we discuss our initiatives for training and upskilling government staff, community mappers, and disaster responders in the use of geospatial technologies. Our work emphasizes the importance of local capacity building in the adoption of AI and Earth Observation for climate resilience in the Caribbean.",
Paul Harris,22 April 2024,3,Envisioning Digital Twins for Instrumented Farms of the Future,"This paper describes the methodological progress in the creation of three demonstrator farm Digital Twins informed by the open and globally unique datasets of three instrumented research farms at Rothamsted Research's North Wyke campus in Devon UK. Mitigating food insecurity, in the context of climate change, is a major challenge of the 21st century. To address this, it is necessary to maintain or enhance productivity from agriculture, while at the same time balancing ecological priorities. To strike this balance, timely decisions, say via a Digital Twin, on agronomic management, system conversion and / or the adoption of a particular farming philosophy are required.",
Arjun Biswas,22 April 2024,4,Using a large language model to create a chatbot companion for a major climate report,"Large language models (LLMs) are text models trained on a large corpus of information and capable of generating natural language responses to user queries . One application of LLMs is to create virtual assistants or chatbots that can provide information about a particular domain, often limiting the LLM to work with a particular corpus or database in a 'retrieval augmented generation' approach. Here we describe the creation and evaluation of a chatbot designed to act as a companion or guide to a new report about tipping points in the climate system.
The Global Tipping Points Report is a recent and comprehensive review of scientific understanding around abrupt changes (so-called tipping points"") in the climate system, including both physical climate and societal shifts. The report was presented at COP28 and incorporated contributions from more than 200 researchers over 90 organizations. We created a chatbot as an additional channel for readers to engage with the large and complex report. The chatbot was intended to be capable of answering questions about report content without going beyond the scope of the report, misrepresenting any content or introducing falsifications. To increase trust, the chatbot was also designed to clearly reference the sections of the report used to generate each answer. Since this was a novel communications mechanism for the report-writing team, an additional goal was to evaluate the chatbot's effectiveness as a way of improving the delivery of scientific information to a general audience.""",
Peter Miersch,22 April 2024,5,Sensitivity analysis of causal discovery on simulated river flood data using non-linear conditional independence testing,"Analyzing river floods with causal inference is a promising pursuit, as it holds the potential to unravel the compounding drivers contributing to them, such as intense precipitation, snowmelt, and elevated antecedent soil moisture. Modern causal inference methods, like the PCMCI (PC algorithm Momentary Conditional Independence) framework, are able to identify such drivers from complex multivariate time series through causal discovery and build causally aware statistical models. However, causal inference tailored to extreme events remains a challenge due to data length limitations, proper conditional independence testing tailored to non-linear relationships and a high level of uncertainty in the resulting causal graphs. In this study, we generate a dataset of simulated high runoff events for many different catchments and perform a sensitivity analysis on these data. We go beyond the typically used linear models by applying Gaussian Process Regression in the conditional independence test to identify the causal graph. We illustrate that Gaussian Process Regression is more suitable to represent the relationship between drivers and response. However, we also show that, even using non-linear independence testing and very long time series, the results do not converge towards a single causal graph. Ultimately, this work will help establish best practices in causal inference for flood research to identify meteorological and catchment specific flood drivers in a changing climate.",
Robert Rouse,22 April 2024,6,Gaussian & Neural Processes in Hydrology,"Machine learning approaches are rapidly changing how modelling is performed in the environmental sciences, with data driven methodologies being slated for operational use in atmospheric and oceanic forecasting.  However, despite the capabilities of machine learning models as empirical models for accurately predicting river system behaviour, they face adoption challenges due to concerns around a perceived lack of interpretability and out-of-sample generalisation.  Gaussian Process models can offer practitioners a more interpretable model that generalises well and enables extrapolation beyond the training data, albeit at high computational cost.  In this research, we investigate the use of Gaussian Processes alongside the related Sparse Variational Gaussian Processes and hybrid Neural Processes and the advantages and disadvantages of these approaches in applying these models to hydrological problems, in terms of general performance, handling extremes, and computational efficiency.",
James Briant,22 April 2024,7,Hybrid climate simulation including machine-learnt subgrid variability from kilometre-scale weather simulation,"Underrepresentation of cloud formation is a known failing in current climate simulations. The coarse grid resolution required by the computational constraint of integrating over long time scales does not permit the inclusion of underlying cloud generating physical processes. This work employs a multi-output Gaussian Process (MOGP) trained on high resolution Unified Model (UM) simulation data to predict the variability of temperature and specific humidity profiles within the climate model. A proof-of-concept study has been carried out where a trained MOGP model is coupled in-situ with a simplified Atmospheric General Circulation Model (AGCM) named SPEEDY. The temperature and specific humidity profiles of the SPEEDY model outputs are perturbed at each timestep according to the predicted high resolution informed variability. 10-year forecasts are generated for both default SPEEDY and ML-hybrid SPEEDY models and output fields are compared ensuring hybrid model predictions remain representative of Earth's atmosphere. Some changes in the precipitation, outgoing longwave and shortwave radiation patterns are observed indicating modelling improvements in the complex region surrounding India and the Indian sea. More generally, this approach creates a computationally efficient path towards more realistic future climate change predictions.",
Ilan Price,22 April 2024,8,Progress and challenges in ensemble weather forecasting with machine learning,Keynote,
Peter Dueben,23 April 2024,9,The digital revolution of Earth system modelling,Keynote,
Oscar Key,23 April 2024,10,Scalable Data Assimilation with Message Passing,"Data assimilation is a core component of numerical weather prediction systems. The large quantity of data processed during assimilation requires the computation to be distributed across increasingly many compute nodes, yet existing approaches suffer from synchronisation overhead in this setting. In this paper, we exploit the formulation of data assimilation as a Bayesian inference problem and apply a message-passing algorithm to solve the spatial inference problem. Since message passing is inherently based on local computations, this approach lends itself to parallel and distributed computation. In combination with a GPU-accelerated implementation, we can scale the algorithm to very large grid sizes while retaining good accuracy and compute and memory requirements.",
Doug McNeall,23 April 2024,11,Will a climate simulation run?       ,"We classify the input parameter space of land surface simulator JULES-ES-1.0 using a random forest classifier, and calculate a probability that the simulator will fail at any chosen input parameter set. We test the predictive power and accuracy of the classifier, using a range of methods. We use the classifier to quantify the importance of each of the input parameters for determining simulator crashes, and discuss what this reveals about the relationship between the simulator and the real system. Finally, we discuss how this informs climate modellers how best to tackle underlying structural errors in the climate simulators.",
Hannah Christensen,23 April 2024,12,Machine Learning for Stochastic Parametrisation,"Atmospheric models used for weather and climate prediction are traditionally formulated in a deterministic manner. In other words, given a particular state of the resolved scale variables, the most likely forcing from the sub-grid scale processes is estimated and used to predict the evolution of the large-scale flow. However, the lack of scale-separation in the atmosphere means that this approach is a large source of error in forecasts. Over recent years, an alternative paradigm has developed: the use of stochastic techniques to characterise uncertainty in small-scale processes. These techniques are now widely used across weather, sub-seasonal, seasonal, and climate timescales. In parallel, recent years have also seen significant progress in replacing parametrisation schemes using machine learning (ML). This has the potential to both speed up and improve our numerical models. However, the focus to date has largely been on deterministic approaches. In this position paper, we bring together these two key developments, and discuss the potential for data-driven approaches for stochastic parametrisation. We highlight early studies in this area, and draw attention to the novel challenges that remain.",
Kenza Tazi,23 April 2024,14,Precipitation prediction from large-scale climatic features over the Upper Indus Basin using Gaussian Processes   ,"Water resources from the Indus Basin sustain over 268 million people. However, water security in this region is threatened by climate change. This is especially the case for the Upper Indus Basin where most solid water reserves are expected to disappear and precipitation to become the main driver of river flow. Yet future precipitation estimates for this region are uncertain. This paper explores the feasibility of using large-scale atmospheric features, which are better predicted by global climate models, to predict local precipitation and assess the probability of extreme precipitation events in the future. More specifically, Gaussian Processes are trained to predict monthly ERA5 precipitation data over a 15-year horizon. This paper also explores different model configurations, including non-stationary covariance functions, and shows that a well-designed Gaussian Process model can be effectively used for extrapolation.",
Marc Girona-Mata,23 April 2024,15,"Spatially-Coherent Probabilistic Downscaling of Daily Precipitation in Ungauged Mountain Locations: A Transfer Learning Study in the Swiss Alps and the Langtang Valley, Nepal","Accurate downscaling of daily precipitation is crucial for water resources and hazard management, especially in regions with complex terrain and a lack of observational data. In such regions, climate reanalysis are not reliable and thus accurate downscaling is usually limited to those locations captured by a network of in-situ measurements instead. Thus, learning to downscale in ungauged locations, whilst maintaining the spatial structure of precipitation, is crucial to effectively downscale (gridded) climate simulations. This study introduces a Gaussian Process - Multi-Layer Perceptron (GP-MLP) latent variable model tailored for the probabilistic downscaling of daily precipitation in ungauged locations. The GP-MLP model consists of an MLP that performs non-linear regression, mapping a set of inputs to distributional parameters of a given probability distribution for each spatio-temporal locations, and we induce spatial correlation between locations with a latent variable modelled by a GP  We jointly learn the GP and MLP parameters using variational inference, which critically allows us to model non-Gaussian probability distributions.
We test our approach in two geographically and climatologically diverse regions: the Swiss Alps and the Langtang Valley in Nepal. The Swiss Alps, with their complex terrain and relatively dense observational network, serve as an ideal region for the initial training of our model. In the Langtang Valley, a high-mountain region with limited ground-based observations, we employ a transfer learning strategy on the model pre-trained in the Swiss Alps. This process involves fine-tuning the model parameters to the unique climatic and topographical features of the Himalayas, thereby enhancing its performance in predicting daily precipitation in this data-sparse region. Our preliminary findings demonstrate the model's strong capability to produce accurate and spatially coherent predictions of daily precipitation for ungauged locations.",
Ilenia Manco,23 April 2024,16,AI-assisted Climate Downscaling of the ERA5 Reanalysis for Rapid Assessment,"The state-of-the-art Global Circulation Models (GCMs) are still operated at such coarse spatial resolutions that necessitate refinement to assess regional climate changes and their impacts. This limitation is primarily attributed to the representation of regional-scale topography and meteorological processes, particularly those associated with extreme events. Conventional dynamical downscaling methods are computationally intensive. In contrast, though computationally efficient, statistical approaches often compromise spatial coherence. To address these limitations, this study introduces an innovative application of Generative Adversarial Networks (GANs). GANs consist of two interconnected components: a generative model and a discriminative model. The generative model, here in represented by the ERA5 climate reanalysis (~31 km), learns to produce high-resolution data. The discriminator, utilizing the VHR-REA_IT dataset (~2.2 km), distinguishes between real high-resolution data and data generated by the GAN (ERA5-DownGAN). This pioneering study utilizes the developed GAN architecture to downscale ERA5 to a resolution of ~2.2 km, particularly for the fields of 2m temperature, 10m wind, and total cumulative precipitation. The training phase (01/1990-12/2000) enables the generative model to learn high-resolution data production, while the testing phase (01/2001-12/2005) evaluates the GAN's performance against VHR-REA_IT. The computational domain focuses on the Italian Peninsula, encompassing parts of northern and central Europe and northern Africa. A set of conventional error metrics, and graphical representations enabling assessment of spatial and temporal correlation between datasets and percentile distribution, is analyzed to evaluate the performance of GANs. The results obtained have shown promise, both in terms of pattern reconstruction and value range (mean, median, extremes), defining this architecture as a potentially viable alternative to dynamical downscaling.",
Peter Manshausen,23 April 2024,17,Predicting visible ship tracks       ,"Aerosol-cloud interactions continue to resist reliable quantification, partly owing to their strong dependence on cloud and weather regimes. For a long time, opportunistic experiments such as ship tracks have been used to overcome issues of confounding. Recent advances leverage (i) Machine Learning (ML) to drastically enlarge ship track data bases, and (ii) 'invisible ship tracks', found by advecting ship emissions, to overcome selection biases in ship track studies. Here, we combine both approaches, to advance our understanding of how meteorology, and emissions amounts, control cloud responses to aerosol. We identify meteorological regimes favourable to the visibility of tracks, using ML methods such as Random Forests. The regime favourable to visible tracks is defined by a stable lower troposphere and little vertical movement, low sea surface temperatures, high cloud cover, and low boundary layer heights.  Building on this relationship, a predictive model like our Random Forest has applications in deliberate Marine Cloud Brightening by predicting the days that are most susceptible to aerosol perturbations.",
Daria Botvynko,23 April 2024,18,Deep Learning for Lagrangian Drift Simulation on Sea Surface,"The simulation of Lagrangian trajectories on the ocean surface holds significance across various application domains, from monitoring plastic and debris movement to investigating algae and plankton dynamics, and forecasting trajectories crucial for search and rescue operations. Assessing the capabilities of ocean numerical models in accurately representing small-scale dynamics is also vital. However, generating realistic trajectories on the sea surface poses a notable scientific challenge within operational oceanography. Model-based methods rely on advection procedures using sea surface velocity fields, yet discrepancies in these fields can lead to inaccurate trajectory modeling. Data-driven learning-based methods have shown promise in capturing spatio-temporal dependencies in simulated trajectories, but few have been applied to the conditional simulation of individual Lagrangian trajectories. Addressing these limitations, this study introduces DriftNet, a novel Deep Learning framework for the conditional simulation of individual trajectories on the sea surface. DriftNet can be trained on any geophysical field containing ocean dynamics information and generates trajectories based on a spatially-explicit latent encoding of targeted trajectory, inspired by Eulerian Fokker-Planck formalism. This approach allows for non-local feature extraction from conditioning input fields, ensuring that the entire dynamics of the surrounding area are considered in modeling the simulated trajectory.",
Anima Anandkumar,23 April 2024,19,Role of AI in tackling climate change,Keynote,
Fiona Turner,24 April 2024,20,Building probabilistic projections of the Antarctic contribution to global sea level rise using a random forest emulator,"We present results from a random forest emulator simultaneously trained on two ice sheet models, Kori and PISM, forced by multiple climate models, producing multi-centennial time series of sea level contribution under two Shared Socioeconomic Pathways (SSPs), SSP1-2.6 and SSP5-8.5. We emulate the relationship between inputs, namely climate change and ice sheet model settings, and an output, sea level contribution. The emulator allows us to interpolate (and extrapolate slightly) in order to build probabilistic projections of sea level contribution to 2300 that include climate and ice sheet modelling uncertainties under all five Shared Socioeconomic Pathways (SSPs), despite only two being used in the ensemble of simulations.",
Leo Edel,24 April 2024,21,Reconstruction of Arctic sea ice thickness (1992-2010) based on a hybrid machine learning and data assimilation approach,"Arctic sea ice thickness (SIT) remains one of the most crucial yet challenging parameters to estimate. Satellite data generally presents temporal and spatial discontinuities, which constrain studies focusing on long-term evolution. Since 2011, the combined satellite product CS2SMOS enables more accurate SIT retrievals that significantly decrease the modelled SIT errors during assimilation. Can we extrapolate the benefits of data assimilation to past periods without SIT observations? In this study, we train a machine learning (ML) algorithm to learn the systematic SIT errors between two versions of the model TOPAZ4 over 2011-2022, with and without CS2SMOS assimilation, to predict the SIT error and extrapolate the SIT prior to 2011. The ML algorithm relies on SIT coming from the two versions of TOPAZ4, various oceanographic variables, and atmospheric forcings from ERA5. Over the test period 2011-2013, the ML method outperforms TOPAZ4 without CS2SMOS assimilation when compared to TOPAZ4 assimilating CS2SMOS. The root mean square error of Arctic averaged SIT decreases from 0.42 to 0.28 meters and the bias from -0.18 to 0.01 meters. Relative to independent mooring data in the Beaufort Gyre between 2001 and 2010, mean SIT bias reduces from 0.21 meters to 0.02 meters when using the ML algorithm. Ultimately, the ML-adjusted SIT reconstruction reveals an Arctic mean SIT of 1.61 meters in 1992 compared to 1.08 meters in 2022. This corresponds to a decline of total sea ice volume from 19,690 to 12,700 km^3 with an associated trend of -3,153 km^3/decade. These changes are accompanied by a distinct shift in SIT distribution. Our innovative approach proves its ability to correct a significant part of the primary biases of the model by combining data assimilation with machine learning. Once this new reconstructed SIT dataset is assimilated in TOPAZ4, the correction can be further propagated to the other sea ice and ocean variables.",
Ayush Prasad,24 April 2024,22,Modeling Snow on Sea Ice using Physics Guided Machine Learning,"Snow is a crucial element of the sea ice system, affecting the sea ice growth and decay due to its low thermal conductivity and high albedo. Despite its importance, present-day climate models have a very idealized representation of snow, often including just 1-layer thermodynamics, omitting several processes that shape its properties. Even though sophisticated snow process models exist, they tend to be excluded in climate modeling due to their prohibitive computational costs. For example, SnowModel is a numerical snow process model developed to simulate the evolution of snow depth and density, blowing-snow redistribution and sublimation, snow grain size, and thermal conductivity, in a spatially distributed, multi-layer snowpack framework. SnowModel can simulate snow distributions on sea ice floes in high spatial (1-m grid) and temporal (1-hour time step) resolution. However, for simulations spanning over large regions, such as the Arctic Ocean, high-resolution runs face challenges of slow processing speeds and the need for large computational resources. To address these common issues in high-resolution numerical modeling, emulators are often used. However, these emulators have their caveats,primarily a lack of generalizability and inconsistency with physical laws. In our study, we address these challenges by using a physics-guided approach in developing our emulator. By integrating physical laws that govern changes
in snow density due to compaction, we aim to create an emulator that is efficient while also adhering to essential physical principles. We evaluated this approach by comparing three machine-learning models, across five distinct Arctic regions. Our results indicate that all models achieved high accuracy, with the Physics-Guided LSTM model demonstrating the most promising results in terms of accuracy and generalizability. Our approach offers a computationally faster way to emulate the SnowModel with high fidelity and a speedup of over 9000 times.",
Harish Baki,24 April 2024,23,Estimating high-resolution profiles of wind speeds from a global reanalysis dataset using TabNet,"The escalating demand for global wind power production, driven by the imperative need for sustainable energy sources, necessitates accurate estimation of vertical wind profiles for efficient wind turbine performance assessment. Traditional methods relying on empirical equations or similarity theory face limitations due to their applicability beyond the surface layer. Recent studies explore Machine Learning (ML) techniques to extrapolate wind speeds, but often focus on single levels, lacking a comprehensive approach to predict entire wind profiles. This study proposes a proof-of-concept in addressing the challenge, utilizing TabNet, an attention-based sequential deep learning model, to predict the entire wind profiles, provided by large-scale meteorological features from reanalysis. To make the methodology generic across datasets, the Chebyshev polynomials are used to approximate the wind profiles with Chebyshev coefficients. Trained on the meteorological features as inputs and the coefficients as targets, the TabNet better predicts unseen wind profiles for different wind conditions, such as high shear, low shear/well mixed, low level jet, and high wind, with a good accuracy. The methodology also addresses the correlation of wind profiles with associated atmospheric conditions by assessing the feature importance. The model demonstrates the feasibility of predicting wind profiles from large-scale meteorological variables, providing a valuable alternative to conventional methods.",
Abhiraami Navaneethanathan,24 April 2024,24,Estimating global ocean POC fluxes through machine learning and data fusion on heterogeneous and sparse in-situ observations,"The ocean biological carbon pump, a significant set of processes in the global carbon cycle, drives the sinking of particulate organic carbon (POC) towards the deep ocean. Models trained to predict global POC fluxes can advance our understanding of how environmental factors influence organic ocean carbon transport, in addition to helping quantify how much carbon is sequestered in the ocean and how nutrients are distributed to different marine ecosystems. POC fluxes can be derived from observations taken by a variety of in-situ instruments such as sediment traps, 234-Thorium tracers and Underwater Vision Profilers. However, the manual and time-consuming nature of data collection poses challenges regarding spatial data sparsity on a global scale, resulting in large estimate uncertainties in under-sampled regions. This research takes an observation-driven approach with machine learning and statistical models trained to estimate POC fluxes globally using in-situ observations and well-sampled environmental driver datasets as predictors, such as temperature and nutrient concentrations. This approach can both fill observational gaps spatiotemporally and reveal the importance of each environmental predictor for estimating POC fluxes. The models built include random forests and neural networks, where their global POC flux estimates, feature importance and model performances are studied and compared. Additionally, this research explores the use of data fusion methods to combine all three heterogeneous in-situ POC flux data sources to achieve improved accuracy and better-informed inferences about organic carbon transport than what is possible using a single data source. By treating the heterogeneous data sources differently, accounting for their uncertainties, and introducing domain knowledge, the proposed Bayesian hierarchical data fusion model can not only harness the information from all three data sources, but also gives insights into their key differences.",
Solomon White,24 April 2024,25,Improving ocean model generalisation by including water type classification as pre-processing to the satellite image,"Understanding and monitoring sea surface physical properties is crucial for gauging ocean health. This is particularly important for the global coastal ocean, which impact coastal communities through provisioning, regulating, supporting, and cultural ecosystem services.  Climate change is leading to an increase in intensity and frequency of marine heat waves, eutrophication and acidification events which not only affect biodiversity but also impact coastal communities through food, human health and economic activities. Ocean colour models predicting sea surface properties such as sea surface salinity (SSS) and temperature (SST) are usually trained using pointwise ground based in-situ data . The model learning relationships between spectral signatures and water column inherent properties. However, this routinely operates at a pixel level and does not include any neighbourhood information   which can inform on spatial distribution of features. The result is a loss of spatial information of the final product. Applying a clustering or segmentation approach to the input image can capture this spatial information by relating pixels to each other by spectral similarity or proximity. This paper aims to determine how using unsupervised learning to create clusters for water classification improves the performance of ocean colour regression models through appropriate algorithm selection as well as retaining spatial information.",
Elena Fillola,24 April 2024,27,Accelerating GHG emissions inference using Graph Neural Networks,"Inverse modelling systems relying on Lagrangian Particle Dispersion Models (LPDMs) are a popular way to quantify greenhouse gas (GHG) emissions using atmospheric observations, providing independent validation to countries' self-reported emissions. However, the increased volume of satellite measurements cannot be fully leveraged due to computational bottlenecks. Here, we propose a data-driven architecture with Graph Neural Networks that emulates the outputs of LPDMs using only meteorological inputs, and demonstrate it in application with results for satellite measurements over Brazil.",
Eliza Duncan,24 April 2024,28,Untangling Aerosol Processes: Exploiting Explainable Machine Learning Techniques in a Lagrangian Framework,"Aerosols represent a major source of uncertainty in climate modelling, compounded by the inherent difficulty in accurately characterising their natural baseline. To investigate the complex relationships between sources, meteorology, and aerosol properties in different rural environments, this study utilises explainable machine learning techniques and a Lagrangian framework. We demonstrate the capability of predicting aerosol properties from airmass history using XGBoost regression models. By employing TreeSHAP to interrogate the models, we demonstrate the effectiveness of these model interrogation techniques to untangle the complex, non-linear processes that govern aerosol properties in rural and pristine regions. This framework allows us to further our understanding of aerosol processes and therefore improve representation in climate models.",
Paolo Pelucchi,24 April 2024,29,Towards probabilistic aerosol retrievals with invertible neural networks,"Satellite remote sensing is the primary source of global aerosol observations, providing essential data for understanding aerosol-climate interactions. To solve the inverse problem at the heart of the retrieval process, traditional algorithms must make simplifications, often introducing bias and neglecting uncertainty. In this study, we explore invertible neural networks (INNs) for retrieving aerosol optical depth (AOD) from top-of-atmosphere reflectance, leveraging their ability to handle under-determined inverse problems and quantify uncertainty. INNs model the forward and inverse processes simultaneously, and use additional random latent variables to recover full non-parametric posterior distributions for the retrievals. We train on synthetic datasets generated by combining radiative transfer model simulations and satellite products. The INNs successfully emulate the forward problem, and inversion results demonstrate accurate retrievals as well, with AOD RMSE aligned with the expected accuracy of the MODIS product. The retrieved posteriors give calibrated uncertainty estimates and offer further insights into retrieval quality. We test the INNs under diverse conditions, exploring their potential to enhance aerosol and climate studies. Challenges and next steps in applying INNs to real satellite observations are discussed",
Amy McGovern,24 April 2024,30,How We are Building Trustworthy AI for Weather and Climate in AI2ES,Keynote,
David-John Gagne,24 April 2024,31,How We are Building Trustworthy AI for Weather and Climate in AI2ES,Keynote,
Marion Weinzierl ,23 April 2024,32,Panel: Perspectives on Practical Reproducibility in Climate Science,"Our panel members are all expert practitioners contributing to access and tools for practical computational reproducibility in climate science. They will share how they got to where they are now, what problems they're currently thinking about with respect to reproducibility, what challenges they've experienced in effecting change to improve reproducibility, and what changes we need to see to improve the fidelity of climate informatics research. ",
Dominic Orchard,23 April 2024,33,Panel: Perspectives on Practical Reproducibility in Climate Science,"Our panel members are all expert practitioners contributing to access and tools for practical computational reproducibility in climate science. They will share how they got to where they are now, what problems they're currently thinking about with respect to reproducibility, what challenges they've experienced in effecting change to improve reproducibility, and what changes we need to see to improve the fidelity of climate informatics research. ",
Alejandro Coca-Castro,23 April 2024,34,Panel: Perspectives on Practical Reproducibility in Climate Science,"Our panel members are all expert practitioners contributing to access and tools for practical computational reproducibility in climate science. They will share how they got to where they are now, what problems they're currently thinking about with respect to reproducibility, what challenges they've experienced in effecting change to improve reproducibility, and what changes we need to see to improve the fidelity of climate informatics research. ",
Elizabeth Barnes ,24 April 2024,35,Panel: Ethics and Explainability in Climate AI: From Theory to Practice,,
David-John Gagne,24 April 2024,36,Panel: Ethics and Explainability in Climate AI: From Theory to Practice,,
Galen McKinley,24 April 2024,37,Panel: Ethics and Explainability in Climate AI: From Theory to Practice,,
Savannah Thais,24 April 2024,38,Panel: Ethics and Explainability in Climate AI: From Theory to Practice,,